#!/usr/bin/env python3
"""
Test Improved Prompts

This script tests the improved prompts generated by the prompt analyzer
against the evaluation tasks to measure improvement.
"""

import json
import sys
from pathlib import Path

# Import evaluation components
from evaluation.evaluator import LLMAgentEvaluator
from evaluation.task_suite import TaskSuite
from optimization.prompt_analyzer import PromptAnalyzer
from agent.llm_agent import LLMCodingAgent, PromptConfig


def test_improved_prompts():
    """Test improved prompts against evaluation tasks."""
    print("ðŸ§ª Testing Improved Prompts")
    print("=" * 50)
    
    # Load the analysis to get improved prompts
    analyzer = PromptAnalyzer("full_evaluation_results.json")
    analysis_report = analyzer.generate_report()
    
    # Get the improved system prompt
    improved_prompts = analysis_report["improved_prompts"]
    improved_system_prompt = improved_prompts["improved_system_prompt"]
    
    print("ðŸ“ Improved System Prompt:")
    print("-" * 30)
    print(improved_system_prompt)
    print()
    
    # Test with a few key tasks
    test_tasks = ["task_1", "task_2", "task_5", "task_7"]  # Mix of easy and complex tasks
    
    print("ðŸ”¬ Testing Improved Prompts on Key Tasks")
    print("=" * 50)
    
    results = []
    
    for task_id in test_tasks:
        print(f"\nðŸ“‹ Testing {task_id}...")
        
        # Create agent with improved prompt
        improved_prompt_config = PromptConfig(
            system_prompt=improved_system_prompt,
            context_inclusion=True,
            action_history_inclusion=True
        )
        
        # Create evaluator and run the task
        evaluator = LLMAgentEvaluator()
        result = evaluator.run_single_task(task_id)
        
        # Store result
        results.append({
            "task_id": task_id,
            "task_name": result.task_name,
            "success": result.success,
            "steps_taken": result.steps_taken,
            "missing_files": result.missing_files,
            "improved_prompt_used": True
        })
        
        # Print result
        status = "âœ… PASS" if result.success else "âŒ FAIL"
        print(f"  {status} - {result.steps_taken} steps")
        if result.missing_files:
            print(f"    Missing: {result.missing_files}")
    
    # Compare with original results
    print("\nðŸ“Š Comparison with Original Results")
    print("=" * 50)
    
    # Load original results
    with open("full_evaluation_results.json", "r") as f:
        original_results = json.load(f)
    
    original_task_results = {
        result["task_id"]: result 
        for result in original_results.get("detailed_results", [])
    }
    
    print("Task | Original | Improved | Change")
    print("-" * 40)
    
    for result in results:
        task_id = result["task_id"]
        original_result = original_task_results.get(task_id, {})
        
        original_success = original_result.get("success", False)
        improved_success = result["success"]
        
        original_status = "âœ…" if original_success else "âŒ"
        improved_status = "âœ…" if improved_success else "âŒ"
        change = "ðŸ”„" if original_success == improved_success else ("ðŸ“ˆ" if improved_success else "ðŸ“‰")
        
        print(f"{task_id} | {original_status} | {improved_status} | {change}")
    
    # Calculate improvement
    improved_success_count = sum(1 for r in results if r["success"])
    original_success_count = sum(1 for r in results if original_task_results.get(r["task_id"], {}).get("success", False))
    
    print(f"\nðŸ“ˆ Improvement Summary:")
    print(f"  Original Success Rate: {original_success_count}/{len(results)} ({original_success_count/len(results)*100:.1f}%)")
    print(f"  Improved Success Rate: {improved_success_count}/{len(results)} ({improved_success_count/len(results)*100:.1f}%)")
    
    if improved_success_count > original_success_count:
        improvement = improved_success_count - original_success_count
        print(f"  ðŸŽ‰ Improvement: +{improvement} tasks")
    elif improved_success_count < original_success_count:
        decline = original_success_count - improved_success_count
        print(f"  ðŸ“‰ Decline: -{decline} tasks")
    else:
        print(f"  âž¡ï¸  No change in success rate")
    
    # Save comparison results
    comparison_data = {
        "test_tasks": test_tasks,
        "results": results,
        "original_results": {
            task_id: original_task_results.get(task_id, {})
            for task_id in test_tasks
        },
        "improvement_summary": {
            "original_success_count": original_success_count,
            "improved_success_count": improved_success_count,
            "total_tasks": len(results)
        }
    }
    
    with open("prompt_improvement_results.json", "w") as f:
        json.dump(comparison_data, f, indent=2, default=str)
    
    print(f"\nðŸ’¾ Comparison results saved to: prompt_improvement_results.json")


if __name__ == "__main__":
    test_improved_prompts() 